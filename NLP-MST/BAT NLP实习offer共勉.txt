https://zhuanlan.zhihu.com/p/36096340
Part I：你的简历

首先你要写一份漂亮的简历，用语简练专业，突出重点，对大部分同学来说压缩成一页就够了，最多两页。
推荐这个免费网站：超级简历，非常简洁漂亮又有迎面而来的大神既视感。
你的简历需要包含这几部分：
1）教育背景：可以写上绩点排名，所在实验室以及研究方向。
2）项目经历：写明问题背景，xx数据集，xx解决方法，具体评价指标达到xx，相比xx方法提升了xx个百分点，最后还可以附上代码开源地址，或者项目上线地址。
3）实习经历：实习所负责的职责，和项目经历一样的写法。
4）论文：如果你投research岗位，顶会是个大加成；如果你投的是nlp工程岗位，论文其实没有硬性要求，更看重项目。
5）荣誉奖项：xx奖学金，kaggle xx牌，xx比赛排名。(打比赛最好是先solo再组队，自己的贡献一目了然）
6）其它：放上你的github地址，blog地址。


其中项目是重点，我相信看到这篇文章的你，应该有相关的NLP项目，那么你应该对你简历所写上去的东西负责任（也就是对细节非常熟），对方可能会问到你：
1）具体参数设置，为什么要这样设置（掌握一下调参玄学）
2）你的模型，为什么这么做，为什么能work，和xx方法比怎么样
3）可能根据你的项目及模型，提出某个可能存在的深藏不露的问题，问你如何解决
4）项目难点是什么，又如何解决，从哪几方面解决，效果提升多少
5）如今的你再来看从前的这个任务，有没有更好的解决思路
6）给你一个新的业务场景，你怎么把你的模型移植上去，怎么重新设计模型，和你之前项目的区别是什么，需要注意哪些问题
7）项目分工，你做了哪部分工作
8）你这个任务的state of the art

Part II：深度学习
这部分其实我没怎么复习，都是经验之谈。
首先关于基础原理，你至少要知道这些：
1）CNN原理，如何用在文本上，在什么情况下适合用CNN，在什么情况下用LSTM
2）RNN系列，掌握RNN、LSTM和GRU的内部结构，RNN产生梯度消失的原因，LSTM如何解决，GRU对LSTM的改进。
3）Word2vec工具，怎么训练词向量，skip-gram和cbow，可以参考一下：一篇通俗易懂的word2vec（也可能并不通俗易懂）
4）Attention机制，比较常见的方法，可以参考一下：Attention用于NLP的一些小结
5）NLP基础任务，比如分词算法（序列标注任务），分类算法

关于实战部分，你至少也要知道这些：
1）数据预处理，权重初始化，为什么不能全部初始化为0，词向量怎么预训练
2）过拟合问题，原因是什么，怎么解决，主要从数据和模型两方面出发：机器学习中用来防止过拟合的方法有哪些？
3）调参技巧，比如，卷积核大小怎么按层设置，bn放在哪里比较合适，激活函数之间的区别（sigmoid，tanh和relu），词向量维度怎么设置，等等。
4）模型评估指标，acc，pre，recall，f1，roc曲线和auc曲线，分别适用于什么任务，怎么降低偏差，怎么降低方差，可以关注一下Hulu微信公众号：Hulu机器学习问题与解答系列 | 第一弹：模型评估
5）优化方法，批量梯度下降，随机梯度下降，mini-batch梯度下降的区别，adam，adagrad，adadelta，牛顿法
6）梯度消失问题，原因（链式求导，激活函数），解决方法（主要是batch norm）；以及梯度爆炸问题（梯度截断）
7）关于训练集和验证集，为什么要划分，如何划分(留出法，交叉验证)
8）如何处理数据不均衡问题，也是从数据和模型两方面出发解决。


其实大部分问题，google一下就有答案。

Part III：传统机器学习
看这本书：周志华-机器学习（俗称西瓜书）
这本书我们实验室人手一本，真正通俗易懂，有基础的话一周就能看得差不多，很多内容我在大三的时候已经学过了。这本书比李航统计学习方法要好看很多，特别适合没有基础的入门者。
我觉得这本书的目录差不多已经涵盖了面试会问到的内容，至少要看完第2-9章，第14章的概率图模型也看一下，第16章的强化学习就见仁见智了，虽然我在简历上写到强化学习背景，但实际上问到我强化学习的只有那么一位面试官（RL+对话系统）

你至少要掌握的算法原理：
1）朴素贝叶斯
2）逻辑回归，线性回归
3）决策树，不同的划分方式，ID3，C4.5，CTAR，XGBoost等等
4）Ensemble模型
5）SVM，核函数选择，不同SVM形式
6）HMM，CRF，如何轻松愉快地理解条件随机场（CRF）？
7）最大熵原理，图解最大熵原理（The Maximum Entropy Principle）
8）KNN和K-Means，DBSACN也了解一下，以及各种距离计算方式，关于机器学习距离的理解
以上列出的算法都需要掌握其基本原理以及优缺点，可以参考：机器学习算法优缺点及其应用领域 - CSDN博客


你必须要会写的公式：
1）BP后向传播过程的推导，可以参考：漫谈LSTM系列的梯度问题，先定义Loss函数，然后分别对输出层参数和隐藏层参数进行求导，得到参数的更新量。
2）softmax和交叉熵推导，分成i=j 和 i[公式]j 两种情况来算，参考这里：大师网-简单易懂的softmax交叉熵损失函数求导
3）各种Loss函数
4）似然函数，负对数似然函数的推导
5）最小二乘法，利用矩阵的秩进行推导
7）贝叶斯定理，拉普拉斯平滑


你最好也要掌握一下的公式：
1）RNN在BP过程中梯度消失的原因，也把这个链式求导过程写出来。
2）各种优化方法的公式，SGD，Momentum，Adagrad，Adam，机器学习优化方法总结比较 - 合唱团abc - 博客园
3）Batch Normalization，就是个归一化过程，再加一个scale操作
4）SVM推导，拉格朗日了解一下：机器学习之拉格朗日乘数法
5）最大熵模型相关推导，一步一步理解最大熵模型 - wxquare - 博客园

Part IV：算法编程
不管你面试什么公司，请记住coding几乎是必考的，这是工程师的基本功。
编程分成三种：普通算法编程，海量数据编程，模型编程。
普通算法编程，一般用C++，需要掌握数组，链表，二叉树，递归，贪心，动态规划，各种容器，各种排序算法，在时间或者空间上的优化思路，以及复杂度的分析。
容器是个好东西，用vector代替数组，用map实现桶思想，用set排序，用queue写bfs，用stack写dfs等等。
推荐大家刷：剑指offer，这本书两天就可以看完（如果仅仅是看题目以及思路），然后上牛客网做一下题：剑指Offer_编程题_牛客网，66道原题全在这，而且评论区有大神出没，某些题的解法我觉得比书上的要巧妙。或者刷LeetCode也可以。

随手列几道常考的代码题：
1）复杂链表的复制，链表的删除
2）最长公共子序列，逆序对
3）快排，归并排序，堆排序
4）二分查找，以及衍生的题目
5）深度优先搜索

海量数据编程，这种用python写比较方便一点，可以把大文件划分成小文件，或者分治加哈希：十道海量数据处理面试题与十个方法大总结 - CSDN博客

模型编程，有时候可能会让你用某个深度学习框架搭某个模型，不过这种比较少。
做科研比较推荐pytorch，业界用tf 比较多，不过也得看组看个人，如果项目需要上线很有可能就是要用tf了。我个人比较喜欢用pytorch，方便搭模型，对RL也很友好。但tf 还是要掌握一下的，指不定哪天这个项目就是要用tf来上线呢。
总之，刷题即可，多写代码多搭模型。
以上都是非常基础的内功，最重要的是你如何向别人去展示你的实力，我觉得只有理解到位了，且有自己的思考，才能和别人进行愉悦的交流和探讨。